{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52617a7-b991-49bc-9527-adddec82e803",
   "metadata": {},
   "source": [
    "## Finetune an LLM for classification\n",
    "- **Code created by**: Noor de Bruijn\n",
    "- **Last edits**: Tuesday June 3rd, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c47ac7-c9cc-4150-8604-60b450af547f",
   "metadata": {},
   "source": [
    "### Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec722ce6-df9b-4a69-b28a-fe53400a163a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Install packages\n",
    "!pip install tqdm\n",
    "!pip install torch\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ccd40-70f5-40bd-9f63-b302f94fa487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798a33f-ada2-46ee-9ac0-79abfef2e0bd",
   "metadata": {},
   "source": [
    "### Check cwd, venv, Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f506d4-22e7-4369-a9cb-2e6b636dd381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check current working directory\n",
    "print(\"Current working directory:\")\n",
    "print(os.getcwd())\n",
    "\n",
    "#Check virtual environment path\n",
    "print(\"\\n Python executable (venv path):\")\n",
    "print(sys.executable)\n",
    "\n",
    "#Check Python version\n",
    "print(\"\\nPython version:\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0085d22-306f-4db9-8960-6df9f120d23e",
   "metadata": {},
   "source": [
    "## Stage 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b44cc7-fc77-4bde-bc32-ffb766fcfb0e",
   "metadata": {},
   "source": [
    "### Step 1. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca794a-2454-449a-b3e0-9b463bbf0fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check size of pkl file\n",
    "!ls -lh \n",
    "\n",
    "#Check specific size\n",
    "print(os.path.getsize(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb99d612-6db2-47ff-bc35-227566786465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define path to file\n",
    "file_path = \"\"\n",
    "\n",
    "try:\n",
    "    obj = joblib.load(file_path)\n",
    "    print(type(obj))\n",
    "except Exception as e:\n",
    "    print(f\"Joblib load error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e68a35-7b81-409e-aa68-d7caa2e79da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Open file\n",
    "with open('', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d4be7-9153-43d0-b0f0-98deaef0df48",
   "metadata": {},
   "source": [
    "### Step 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6b1ec-0899-496e-be2b-7344e06269bb",
   "metadata": {},
   "source": [
    "#### Split the dataset (train, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e50cbf-91d1-427c-84fd-3631375c7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to create three datasets (train, validation, test)\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "    \n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "#Apply function to current dataframe\n",
    "train_df, validation_df, test_df = random_split(df, 0.7, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070eedd-38dd-4b61-86aa-5d26b3f1cac5",
   "metadata": {},
   "source": [
    "#### Save datasets as separate CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f4672-d9bb-459a-a004-fc25430725aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the datasets as CSV files so they can be used later\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9a9e6-b35b-44b2-89fb-8e89299ef66d",
   "metadata": {},
   "source": [
    "### Step 3. Create data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b497dc06-b028-4687-a51d-a11a107bf6e7",
   "metadata": {},
   "source": [
    "#### Implement a PyTorch Dataset (specifies how data is loaded and preprocessed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03105a39-b7fe-44ca-acc1-d3f029841fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
    "        \n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "            else:\n",
    "                self.max_length = max_length\n",
    "                \n",
    "                #Truncuation if text > max_length\n",
    "                self.encoded_texts = [encoded_texts[:self.max_length] for encoded_text in self.encoded_texts]\n",
    "                \n",
    "                #Padding with <|endoftext|> = 50256 (GPT2)\n",
    "                self.encoded_texts = [\n",
    "                    encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "                    for encoded_text in self.encoded_texts\n",
    "                ]\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.data))\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2668c6d6-5cfe-4c3e-b615-5a8ae11b4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply DS to train.csv\n",
    "train_dataset = DS(csv_file = \"train.csv\",\n",
    "                   max_length = None,\n",
    "                   tokenizer = tokenizer\n",
    "                  )\n",
    "\n",
    "#Apply DS to validation.csv\n",
    "validation_dataset = DS(csv_file = \"validation.csv\",\n",
    "                   max_length = None,\n",
    "                   tokenizer = tokenizer\n",
    "                  )\n",
    "\n",
    "#Apply DS to test.csv\n",
    "test_dataset = DS(csv_file = \"test.csv\",\n",
    "                   max_length = None,\n",
    "                   tokenizer = tokenizer\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de500fa-8989-4648-bf96-a2b41dd2e629",
   "metadata": {},
   "source": [
    "#### Creating PyTorch data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ff3f5-b372-42e9-bff8-b198159642dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting various variables\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "#Dataloader for train_dataset\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "#Dataloader for validation_dataset\n",
    "validation_loader = DataLoader(\n",
    "    dataset=validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "#Dataloader for test_dataset\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa37ddc-2673-45c6-94d3-d0d2089dbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print statement to see number of batches in each dataset\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(validation_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3a03f-7b7a-4720-befc-6e884290d7fd",
   "metadata": {},
   "source": [
    "## Stage 2. Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7ac6c-3745-4176-8713-b9df49e00d5e",
   "metadata": {},
   "source": [
    "### Step 4. Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50f52d-fc10-4724-a187-b630b802f456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Select model\n",
    "MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Example text\"\n",
    "\n",
    "#Base model configuration\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "#Define model specific parameters (embedding dimension, # of transformer blocks, # of attention heads)\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "#Using .update to add new keys from model_configs[MODEL] to BASE_CONFIG\n",
    "BASE_CONFIG.update(model_configs[MODEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d88de-5ead-42aa-9ef3-b47f805b62a0",
   "metadata": {},
   "source": [
    "### Step 5. Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db82b1bf-1058-4262-b463-3de53e03fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device variable to GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a06c2-5311-4de7-9a45-b6d65d633839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPTModel\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b985f0-30ae-48f5-b400-f322ddff6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to download original GPT2 weights\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path, backup_url)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb884a-8d78-4df2-a854-6d5618b67b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually load pre-trained weights into custom GPT model (gpt) from set of stored parameters (params)\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a168a-3bd4-4dfa-85ff-c3598dd3c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model and model size\n",
    "MODEL = \"gpt2-small (124M)\"\n",
    "model_size = \"124M\"\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=\"gpt2\"\n",
    ")\n",
    "#Define the model\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "#Load the weights\n",
    "load_weights_into_gpt(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1285704-c0a1-4678-ade7-6c8d4f3fcd96",
   "metadata": {},
   "source": [
    "### Step 6. Modify model for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e957e-beaf-4471-8047-ee68b9d531ff",
   "metadata": {},
   "source": [
    "#### Adding a classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d3668-e049-44be-b650-6e219d180620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed and num_classes variable\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "\n",
    "#Change out_head of the model\n",
    "#This new output_layer has required_grad automatically set to True\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"],\n",
    "    out_features = num_classes\n",
    ")\n",
    "\n",
    "#Make transformer block and final normalization layer trainable (set required_grad = True)\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.required_grad = True\n",
    "for param in model.final_norm.parameters():\n",
    "    param.required_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513d52b-9de5-43c5-a4ee-f5d3cef38fd1",
   "metadata": {},
   "source": [
    "### Step 7. Implement evaluation utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e932462-0edb-417b-98a5-374f008fa603",
   "metadata": {},
   "source": [
    "#### Define functions for classification accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a38745-db7d-47ea-8932-ee65f958f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. Classification accuracy\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct predictions, num_examples = 0, 0\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "                predicted_labels = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                num_examples += predicted_labels.shape[0]\n",
    "                correct_predictions += (\n",
    "                    (predicted_labels == target_batch).sum().item()\n",
    "                )\n",
    "                \n",
    "                else:\n",
    "                    break\n",
    "            return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd45de7-b719-4e3c-90d1-939088ac1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# II. Classification loss\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f545f-9a26-4f7f-85a1-dba7c3f0d5d9",
   "metadata": {},
   "source": [
    "#### Determine classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4efd69-9ace-4d4c-850d-fe6f7098f4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set device variable to GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#Set number of batches\n",
    "num_batches = 10\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "#Determine classification accuracy across train, validation, test datasets\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=num_batches\n",
    ")\n",
    "\n",
    "validation_accuracy = calc_accuracy_loader(\n",
    "    validation_loader, model, device, num_batches=num_batches\n",
    ")\n",
    "\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=num_batches\n",
    ")\n",
    "\n",
    "#Print statements to see accuracy across different datasets\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {validation_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "#Important: To improve prediction accuracies, we need to fine-tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277cd63-27c2-489d-af0d-8ec79685d9c7",
   "metadata": {},
   "source": [
    "#### Determine classification loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad96ad-ae46-4f2e-bfb8-4852b85d1005",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=num_batches)\n",
    "    val_loss = calc_loss_loader(validation_loader, model, device, num_batches=num_batches)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=num_batches)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613315bb-888e-4e18-8539-efd5caf9d668",
   "metadata": {},
   "source": [
    "## Stage 3. Model fine-tuning and usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab10d48-0b8c-45a1-a57e-06e8904a5898",
   "metadata": {},
   "source": [
    "### Step 8. Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ed2a6-b0c9-4adc-a842-b93556fc1f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_classifier_simple(\n",
    "        model, train_loader, validation_loader, optimizer, device,\n",
    "        num_epochs, eval_freq, eval_iter):\n",
    "    train_losses, validation_losses, train_accuracies, validation_accuracies = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        #Training loop\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "            \n",
    "            #Calculate losses\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, validation_loss = evaluate_model(\n",
    "                    model, train_loader, validation_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                validation_losses.append(validation_loss)\n",
    "                \n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Validation loss {validation_loss:.3f}\"\n",
    "                      \n",
    "        #Calculate accuracies        \n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        validation_accuracy = calc_accuracy_loader(validation_loader, model, device, num_batches=eval_iter)\n",
    "        \n",
    "        #Print accuracies\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "        print(f\"Validation accuracy: {validation_accuracy*100:.2f}%\")\n",
    "        \n",
    "        #Append accuracies to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "    return train_losses, validation_losses, train_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d57c8-0421-48bd-ac71-eac37859caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, validation_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        validation_loss = calc_loss_loader(validation_loader, model, device, num_batches=eval_iter)\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    return train_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4508b-c80a-40e9-98b6-25986b3e4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize various variables\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "\n",
    "#Initiate training\n",
    "train_losses, validation_losses, train_accuracies, validation_accuracies, examples_seen = \\\n",
    "    train_classifier_simple(\n",
    "        model, train_loader, validation_loader, optimizer, device, \n",
    "        num_epochs=num_epochs, eval_freq=50, eval_iter=5\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time-start_time) / 60\n",
    "\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd171ae1-a42b-4d16-a7bc-0726b18e62ff",
   "metadata": {},
   "source": [
    "### Step 9. Evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbafbbb-8eb2-430a-97a2-04324a2c84dc",
   "metadata": {},
   "source": [
    "#### Plot classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5deff3-4d92-4674-b616-8554c5980ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_values(\n",
    "    epochs_seen, examples_seen, train_values, validation_values, label = \"loss\"):\n",
    "    fig, ax1 = plot.subplots(figsize=(3,5))\n",
    "    \n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, validation_values, linestyle = \"-.\", label=f\"Validation {label}\")\n",
    "    \n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    as1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b949a6-1593-42c1-a84b-c676b56b8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two 1D tensors (#torch.linspace(start, end, steps))\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "#Create plot\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71e074-06ea-471e-87f8-1f0b9bc9b86b",
   "metadata": {},
   "source": [
    "#### Plot classification accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bac4f9-1946-4275-8850-135326f31317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two 1D tensors\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accuracies))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accuracies))\n",
    "\n",
    "#Create plot\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accuracies, validation_accuracies, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2352a9-b1df-4664-b30f-9d4e8af305d0",
   "metadata": {},
   "source": [
    "#### Calculate performance metrics for training, validation and test sets across entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a9112-96a0-42be-abbf-143d4b03ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "validation_accuracy = calc_accuracy_loader(validation_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {validation_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6f7c8-dc74-49ed-81fe-2083cb928baa",
   "metadata": {},
   "source": [
    "### Step 10. Use model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490a1f9-42d9-496b-abb8-13c2fb76dbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
